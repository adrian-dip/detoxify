{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def mappingfn1(x):\n",
    "    if x < 0.01:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def mappingfn2(x):\n",
    "    if x < 0.099999:\n",
    "        return 0\n",
    "    if x > 0.099999:\n",
    "        return 1\n",
    "\n",
    "def zeroes(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    \n",
    "def excluder(x):\n",
    "    if x == 1:\n",
    "        return x\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords_list = stopwords\n",
    "    word_tokens = word_tokenize(text)\n",
    "    result = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stopwords_list:\n",
    "            result.append(w)\n",
    "    return ' '.join(result)\n",
    "\n",
    "def remove_special(text, lower=True):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = \" \".join(\n",
    "        text.split()\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def remove_repeat(text, repeat=1):\n",
    "    text = text.split(' ')\n",
    "    result = []\n",
    "    for word in text:\n",
    "        if result.count(word)<repeat:\n",
    "            result.append(word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379518"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build two dataframes and fit vectorizer on the negative class (much more frequent in production) \n",
    "# to avoid overfitting and increase the robustness of model \n",
    "\n",
    "trainpos = pd.read_csv('train_stemmed.csv')\n",
    "trainpos.fillna(0, inplace=True)\n",
    "trainpos['effectiveness'] = trainpos['target'].apply(mappingfn2)\n",
    "trainpos['effectiveness'] = trainpos['effectiveness'].apply(zeroes)\n",
    "trainpos.dropna(inplace=True)\n",
    "trainpos = trainpos.sample(frac=0.6).reset_index(drop=True)\n",
    "\n",
    "train = pd.read_csv('train_stemmed.csv')\n",
    "train.fillna(0, inplace=True)\n",
    "train['effectiveness'] = train['target'].apply(mappingfn2)\n",
    "train['effectiveness'] = train['effectiveness'].apply(excluder)\n",
    "train.dropna(inplace=True)\n",
    "train = train.sample(frac=0.95).reset_index(drop=True)\n",
    "train = pd.concat([trainpos, train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stopword list\n",
    "\n",
    "cv = CountVectorizer(ngram_range = (1, 1)) \n",
    "documents = train.comment_text.tolist()\n",
    "documents = [' '.join(documents)] \n",
    "\n",
    "X = cv.fit_transform(documents).toarray()\n",
    "freqs = X.flatten() \n",
    "words = cv.get_feature_names() \n",
    "\n",
    "df_word = pd.DataFrame({'word': words, 'freq': freqs})\n",
    "df_word = df_word.sort_values(by='freq', ascending=False)\n",
    "\n",
    "df_word = df_word.reset_index().drop(['index'],axis=1)\n",
    "\n",
    "stopwords_list = df_word.word.tolist()[:150]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_list += stopwords.words('english')\n",
    "stopwords = set(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build test set\n",
    "\n",
    "test_dic = dict(zip(train.id, [0] * len(train.id)))\n",
    "\n",
    "def exclude_test_set(x):\n",
    "    if x not in test_dic:\n",
    "        return x\n",
    "\n",
    "train['preprocess_text'] = train['preprocess_text'].apply(lambda x: str(x))\n",
    "train2 = pd.read_csv('train_stemmed.csv')\n",
    "train2['id'] = train2['id'].apply(exclude_test_set)\n",
    "train2.dropna(inplace=True)\n",
    "train2['preprocess_text'] = train2['preprocess_text'].apply(lambda x: str(x))\n",
    "preprocess_text2 = train2.preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer & split the data\n",
    "# We use a very small test size as the models have been cross-validated on a 4-fold split beforehand. \n",
    "\n",
    "preprocess_text = train.preprocess_text\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000, ngram_range = (1, 2))\n",
    "\n",
    "X = vectorizer.fit_transform(preprocess_text)\n",
    "\n",
    "y = train.target\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.0001, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vectorizer_final.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "----prediction of {} column----\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97    258550\n",
      "           1       0.46      0.63      0.53     13304\n",
      "\n",
      "    accuracy                           0.95    271854\n",
      "   macro avg       0.72      0.80      0.75    271854\n",
      "weighted avg       0.96      0.95      0.95    271854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' ')\n",
    "print('----prediction of {} column----')\n",
    "print(' ')\n",
    "y = np.where(y_train >= 0.3, 1, 0)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y)\n",
    "y_pred = model.predict(X_valid)\n",
    "X = vectorizer.transform(preprocess_text2)\n",
    "y = np.where(train2['target'] >= 0.3, 1, 0)\n",
    "y_pred1 = model.predict(X)\n",
    "print(classification_report(y, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logisticRegression03.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "----prediction of {} column----\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98    261652\n",
      "           1       0.56      0.57      0.57     10202\n",
      "\n",
      "    accuracy                           0.97    271854\n",
      "   macro avg       0.77      0.78      0.77    271854\n",
      "weighted avg       0.97      0.97      0.97    271854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' ')\n",
    "print('----prediction of {} column----')\n",
    "print(' ')\n",
    "y = np.where(y_train >= 0.4, 1, 0)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y)\n",
    "y_pred = model.predict(X_valid)\n",
    "X = vectorizer.transform(preprocess_text2)\n",
    "y = np.where(train2['target'] >= 0.4, 1, 0)\n",
    "y_pred1 = model.predict(X)\n",
    "print(classification_report(y, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logisticRegression04.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "----prediction of {} column----\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    264457\n",
      "           1       0.62      0.51      0.56      7397\n",
      "\n",
      "    accuracy                           0.98    271854\n",
      "   macro avg       0.80      0.75      0.78    271854\n",
      "weighted avg       0.98      0.98      0.98    271854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' ')\n",
    "print('----prediction of {} column----')\n",
    "print(' ')\n",
    "y = np.where(y_train >= 0.5, 1, 0)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y)\n",
    "y_pred = model.predict(X_valid)\n",
    "X = vectorizer.transform(preprocess_text2)\n",
    "y = np.where(train2['target'] >= 0.5, 1, 0)\n",
    "y_pred1 = model.predict(X)\n",
    "print(classification_report(y, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logisticRegression05.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.91    258550\n",
      "           1       0.21      0.75      0.33     13304\n",
      "\n",
      "    accuracy                           0.85    271854\n",
      "   macro avg       0.60      0.80      0.62    271854\n",
      "weighted avg       0.95      0.85      0.89    271854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = np.where(y_train >= 0.3, 1, 0)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train,y)\n",
    "y_pred = model.predict(X_valid)\n",
    "X = vectorizer.transform(preprocess_text2)\n",
    "y = np.where(train2['target'] >= 0.3, 1, 0)\n",
    "y_pred3 = model.predict(X)\n",
    "print(classification_report(y, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('multinomialNB03.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    264457\n",
      "           1       0.63      0.48      0.54      7397\n",
      "\n",
      "    accuracy                           0.98    271854\n",
      "   macro avg       0.81      0.73      0.76    271854\n",
      "weighted avg       0.98      0.98      0.98    271854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = np.where(y_train >= 0.5, 1, 0)\n",
    "model = SGDClassifier()\n",
    "model.fit(X_train,y)\n",
    "y_pred = model.predict(X_valid)\n",
    "X = vectorizer.transform(preprocess_text2)\n",
    "y = np.where(train2['target'] >= 0.5, 1, 0)\n",
    "y_pred6 = model.predict(X)\n",
    "print(classification_report(y, y_pred6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SGD05.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1420520e17844219c9a674b6f7e48cf34564d58f6fd438366edf503ca13776c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
